{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iguds\\AppData\\Local\\Temp\\ipykernel_8608\\4113585306.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard SummaryWriter\n",
    "import cv2\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_policy = 0\n",
    "good_policy = 0\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_NUM_EPISODES = 30000\n",
    "ALPHA = 1e-4\n",
    "GAMMA = .99\n",
    "batch_size = 64\n",
    "capacity = 1000000\n",
    "sync_frequency = 10000\n",
    "use_target_network = True\n",
    "epsilon = 1\n",
    "def Epsilon(episode):\n",
    "    return np.exp(-3*episode/MAX_NUM_EPISODES)\n",
    "\n",
    "l,g,m1,m2,t=sp.symbols('l,g,m1,m2,t')\n",
    "theta,s,a = sp.symbols('theta,s,a',cls=sp.Function)\n",
    "theta,s,a = theta(t),s(t),a(t)\n",
    "thetadot,sdot = sp.diff(theta,t),sp.diff(s,t)\n",
    "thetaddot,sddot = sp.diff(thetadot,t),sp.diff(sdot,t)\n",
    "x = s + l*sp.sin(theta)\n",
    "y = -l*sp.cos(theta)\n",
    "T = 0.5*m1*sdot**2 + 0.5*m2*(sp.diff(x,t)**2 + sp.diff(y,t)**2)\n",
    "U = m2 *g * y\n",
    "L = T - U\n",
    "LE1 = sp.diff(L,s) - sp.diff(sp.diff(L,sdot),t) + a\n",
    "LE2 = sp.diff(L,theta) - sp.diff(sp.diff(L,thetadot),t)\n",
    "sols = sp.solve([LE1,LE2],(sddot,thetaddot))\n",
    "sddotlambda = sp.lambdify((m1,m2,l,g,a,s,theta,sdot,thetadot),sols[sddot])\n",
    "thetaddotlambda = sp.lambdify((m1,m2,l,g,a,s,theta,sdot,thetadot),sols[thetaddot])\n",
    "\n",
    "def diffeq(y,t,m1,m2,l,g,a):\n",
    "  s,sdot,theta,thetadot = y\n",
    "  sddot = sddotlambda(m1,m2,l,g,a,s,theta,sdot,thetadot)\n",
    "  thetaddot = thetaddotlambda(m1,m2,l,g,a,s,theta,sdot,thetadot)\n",
    "  return [sdot,sddot,thetadot,thetaddot]\n",
    "\n",
    "\n",
    "class Cartpendflip(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(Cartpendflip, self).__init__()\n",
    "\n",
    "        self.action_space = spaces.Discrete(20)\n",
    "        self.action_shape = self.action_space.n\n",
    "        self.observation_space = spaces.Box(low=np.array([-10,-1000,-10*np.pi,-10*np.pi], dtype=np.float32),high=np.array([10,1000,10*np.pi,10*np.pi], dtype=np.float32))\n",
    "        self.state = np.array([0.0, 0.0, 0.0])\n",
    "        self.properties = {'m1': 1, 'm2': 1, 'l': 1, 'g': 9.8}\n",
    "        self.dt = 0.05\n",
    "        self.goalstate = np.array([0,0,np.pi,0])\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "\n",
    "    def ActiontoForce(self,action):\n",
    "      force = 20*(self.properties['m1']+self.properties['m2'])*(action / (self.action_shape -1) -0.5)\n",
    "      return force\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0 \n",
    "        force = self.ActiontoForce(action)\n",
    "        self.state = odeint(diffeq,self.state,[0,self.dt],args = tuple([self.properties[k] for k in ['m1','m2','l','g']] + [force]))[1]\n",
    "        self.current_step += 1\n",
    "        if  self.current_step >= self.max_steps or np.abs(self.state[0]) > 9 or np.abs(self.state[3]) > 20 or np.abs(self.state[2]) > 9*np.pi:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "\n",
    "        s,sdot,theta,thetadot = self.state\n",
    "        sopt,sdotopt,thetaopt,thetadotopt = self.goalstate\n",
    "        if theta > np.pi- 0.3 and theta <np.pi + 0.3:\n",
    "            reward = 1\n",
    "\n",
    "        return self.state, reward, done,{}, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment state and steps\n",
    "        self.state = np.random.uniform(-1,1,size=4)\n",
    "        self.current_step = 0\n",
    "        return self.state,_\n",
    "    \n",
    "    \n",
    "env = Cartpendflip()\n",
    "\n",
    "class SLP(torch.nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, device=device, hidden_shape1=64, hidden_shape2=64):\n",
    "        super(SLP, self).__init__()\n",
    "        self.device = device\n",
    "        self.input_shape = input_shape[0]\n",
    "        self.output_shape = output_shape\n",
    "        self.hidden_shape1 = hidden_shape1\n",
    "        self.hidden_shape2 = hidden_shape2\n",
    "        self.linear1 = torch.nn.Linear(self.input_shape, self.hidden_shape1)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_shape1, self.output_shape)\n",
    "        #self.linear2 = torch.nn.Linear(self.hidden_shape1, self.hidden_shape2)\n",
    "        #self.linear3 = torch.nn.Linear(self.hidden_shape2, self.output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=self.device)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        #x = self.linear3(x)\n",
    "        # x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.absolutebellmanerror = deque(maxlen=capacity)\n",
    "\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
    "        obs,next_obs = np.array(obs),np.array(next_obs)\n",
    "        experiences = [torch.Tensor(obs), \n",
    "                        torch.Tensor(actions), \n",
    "                        torch.Tensor(rewards), \n",
    "                        torch.Tensor(next_obs), \n",
    "                        torch.Tensor(dones)]\n",
    "        for i, thing in enumerate(experiences):\n",
    "            experiences[i] = thing.to(device)\n",
    "        return experiences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class Q_Learner(object):\n",
    "    def __init__(self, env):\n",
    "        self.obs_shape = env.observation_space.shape\n",
    "        self.obs_high = env.observation_space.high\n",
    "        self.obs_low = env.observation_space.low\n",
    "        self.action_shape = env.action_space.n\n",
    "        self.Q = SLP(self.obs_shape, self.action_shape).to(device)\n",
    "        self.best_policy = self.Q\n",
    "        \n",
    "\n",
    "        if use_target_network:\n",
    "            self.target_Q = SLP(self.obs_shape, self.action_shape).to(device)\n",
    "            self.update_frequency = sync_frequency\n",
    "            self.step = 0\n",
    "        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=ALPHA)\n",
    "        self.gamma = GAMMA\n",
    "        self.Q_loss = torch.nn.MSELoss()\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if np.random.random() > epsilon:\n",
    "            return torch.argmax(self.Q(obs).data).item()\n",
    "        else:\n",
    "            return np.random.choice([a for a in range(self.action_shape)])\n",
    "\n",
    "    def learn(self, obs, actions, rewards, next_obs, dones):\n",
    "        # Get Q-values for current states (for the actions taken)\n",
    "        if use_target_network:\n",
    "            if self.step % self.update_frequency == 0:\n",
    "                self.target_Q.load_state_dict(self.Q.state_dict())\n",
    "            next_actions = self.Q(next_obs).argmax(1)\n",
    "            td_target = rewards + self.gamma * self.target_Q(next_obs).gather(1,next_actions.unsqueeze(1).long()).squeeze(1) * (1 - dones)\n",
    "            self.step += 1\n",
    "        else:\n",
    "            # Compute TD target: reward + gamma * max(next_q_value) * (1 - done)\n",
    "\n",
    "            td_target = rewards + self.gamma * self.Q(next_obs).max(1)[0] * (1 - dones)\n",
    "\n",
    "        current_q_values = self.Q(obs).gather(1, actions.unsqueeze(1).long()).squeeze(1)\n",
    "        # Compute the loss\n",
    "        tderror = self.Q_loss(current_q_values, td_target)\n",
    "\n",
    "        # Perform backpropagation and optimization step\n",
    "        self.Q_optimizer.zero_grad()\n",
    "        tderror.backward()\n",
    "        self.Q_optimizer.step()\n",
    "\n",
    "def train(agent, env, replay_buffer, writer):\n",
    "    global epsilon\n",
    "    best_reward = -float('inf')\n",
    "    last100totalrewards = deque(maxlen=100)\n",
    "    goodreward = -float('inf')\n",
    "    for episode in range(MAX_NUM_EPISODES):\n",
    "        done = False\n",
    "        obs = env.reset()[0]\n",
    "        total_reward = 0.0\n",
    "        epsilon = Epsilon(episode)\n",
    "        step = 0\n",
    "        while not done:\n",
    "            step += 1\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, done,truncated, info = env.step(action)\n",
    "            replay_buffer.add([obs, action, reward, next_obs, done])\n",
    "            obs = next_obs\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                agent.learn(*replay_buffer.sample(batch_size))\n",
    "\n",
    "            total_reward += reward\n",
    "            \n",
    "            if step > 400:\n",
    "                break\n",
    "\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_policy = agent.get_action\n",
    "            agent.best_policy = best_policy\n",
    "        if episode == MAX_NUM_EPISODES - 50:\n",
    "            if total_reward > goodreward:\n",
    "                goodreward = total_reward\n",
    "                good_policy = agent.get_action\n",
    "\n",
    "        last100totalrewards.append(total_reward)\n",
    "        average_reward = np.mean(last100totalrewards)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Total Reward', total_reward, episode)\n",
    "        writer.add_scalar('Best Reward', best_reward, episode)\n",
    "        writer.add_scalar('Epsilon', epsilon, episode)\n",
    "        writer.add_scalar('last100totalrewardsaverage', average_reward, episode)\n",
    "        writer.add_scalar('numstepsperepisode', step, episode)\n",
    "\n",
    "    return best_policy, good_policy\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter('runs/cartpole_experiment')  # Specify the directory for logs\n",
    "# Load TensorBoard in Colab\n",
    "# After training, load TensorBoard directly in the notebook\n",
    "\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity)\n",
    "agent = Q_Learner(env)\n",
    "policy,good_policy = train(agent, env, replay_buffer, writer)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "%matplotlib tk\n",
    "# Assuming the Cartpendflip environment is defined and initialized properly.\n",
    "env = Cartpendflip()\n",
    "l = env.properties['l']  # Retrieve the length from the environment properties\n",
    "MaxNumSteps = 1000\n",
    "ss = []\n",
    "xs = []\n",
    "ys = []\n",
    "obs = env.reset()[0]\n",
    "\n",
    "for step in range(MaxNumSteps):\n",
    "    action = policy(obs)  # Get action from the policy\n",
    "    obs, reward, done,_, info = env.step(action)\n",
    "\n",
    "    s, theta = obs[0], obs[2]\n",
    "    x = s + l * np.sin(theta)\n",
    "    y = -l * np.cos(theta)\n",
    "\n",
    "    ss.append(s)\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "\n",
    "    if done:  # Check if the episode is done\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(-1, 7)\n",
    "ax.set_ylim(-4, 4)\n",
    "\n",
    "cart_width = 0.4\n",
    "cart_height = 0.2\n",
    "# Create patches\n",
    "cart_patch = Rectangle((ss[0] - cart_width / 2, -cart_height / 2), cart_width, cart_height, fc='blue')\n",
    "ax.add_patch(cart_patch)  # Add cart as a rectangle\n",
    "\n",
    "# Plot pendulum elements\n",
    "pendulum, = ax.plot([], [], 'r-', lw=2)  # Red line for the pendulum\n",
    "\n",
    "# Animation function\n",
    "def animate(i):\n",
    "    # Update cart position using the current value of s[i]\n",
    "    cart_patch.set_xy((ss[i] - cart_width / 2, -cart_height / 2))\n",
    "\n",
    "    # Pendulum (line from the cart to the bob)\n",
    "    pendulum_x = [ss[i], xs[i]]  # From cart center to pendulum bob\n",
    "    pendulum_y = [0, ys[i]]     # From cart top to pendulum bob\n",
    "    pendulum.set_data(pendulum_x, pendulum_y)\n",
    "\n",
    "    return cart_patch, pendulum\n",
    "\n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig, animate, frames=len(ss), interval=50, blit=True)\n",
    "ani.save('flipupcartpend.gif',writer='pillow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Assuming the Cartpendflip environment and policy are defined and initialized properly\n",
    "env = Cartpendflip()\n",
    "l = env.properties['l']  # Retrieve the length of the pendulum from environment properties\n",
    "MaxNumSteps = 1000\n",
    "rows, cols = 6, 6  # Number of rows and columns in the subplot grid\n",
    "\n",
    "# Function to simulate a single Cartpendflip episode\n",
    "def simulate_episode():\n",
    "    ss, xs, ys = [], [], []\n",
    "    obs = env.reset()[0]\n",
    "    \n",
    "    for step in range(MaxNumSteps):\n",
    "        action = policy(obs)  # Get action from the policy\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "\n",
    "        s, theta = obs[0], obs[2]\n",
    "        x = s + l * np.sin(theta)\n",
    "        y = -l * np.cos(theta)\n",
    "\n",
    "        ss.append(s)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return ss, xs, ys\n",
    "\n",
    "# Collect simulation data for each subplot\n",
    "sim_data = [simulate_episode() for _ in range(rows * cols)]\n",
    "env.close()  # Close environment after data collection\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(rows, cols, figure=fig)\n",
    "subplots = []\n",
    "\n",
    "# Set up subplots and animation elements for each subplot\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        ax = fig.add_subplot(gs[r, c])\n",
    "        ax.set_xlim(-2, 8)\n",
    "        ax.set_ylim(-5, 5)\n",
    "\n",
    "        cart_width = 0.4\n",
    "        cart_height = 0.2\n",
    "        cart_patch = Rectangle((0, -cart_height / 2), cart_width, cart_height, fc='blue')\n",
    "        ax.add_patch(cart_patch)\n",
    "\n",
    "        pendulum, = ax.plot([], [], 'r-', lw=2)\n",
    "        subplots.append((ax, cart_patch, pendulum))\n",
    "\n",
    "# Animation function for updating each subplot\n",
    "def animate(i):\n",
    "    for index, (ax, cart_patch, pendulum) in enumerate(subplots):\n",
    "        ss, xs, ys = sim_data[index]\n",
    "        \n",
    "        if i < len(ss):\n",
    "            # Update cart position for each subplot\n",
    "            cart_patch.set_xy((ss[i] - cart_width / 2, -cart_height / 2))\n",
    "            \n",
    "            # Update pendulum position\n",
    "            pendulum_x = [ss[i], xs[i]]\n",
    "            pendulum_y = [0, ys[i]]\n",
    "            pendulum.set_data(pendulum_x, pendulum_y)\n",
    "    \n",
    "    return [patch for _, patch, _ in subplots] + [pendulum for _, _, pendulum in subplots]\n",
    "\n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig, animate, frames=MaxNumSteps, interval=50, blit=True)\n",
    "ani.save('epic.gif',writer='pillow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
